{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f63a7e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Student Dropout Prediction Pipeline\n",
    "----------------------------------\n",
    "This notebook implements a robust, production-ready pipeline for predicting student dropout using the UCI dataset and PyCaret. It covers data cleaning, feature engineering, model training, evaluation, prediction, and feature importance extraction, with explanations for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eabebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import display\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pycaret.classification import setup, compare_models, pull, save_model, evaluate_model, predict_model, get_config, finalize_model, plot_model\n",
    "import shap\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Key categorical mappings from variable_list.md\n",
    "GENDER_MAP = {1: 'male', 0: 'female'}\n",
    "MARITAL_STATUS_MAP = {\n",
    "    1: 'single', 2: 'married', 3: 'widower', 4: 'divorced', 5: 'facto union', 6: 'legally separated'\n",
    "}\n",
    "TARGET_MAP = {0: 'Dropout', 1: 'Enrolled', 2: 'Graduate'}  # Adjust if actual codes differ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d2151",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Data Acquisition & Understanding\n",
    "We fetch the dataset and display its structure. Understanding the data is the first step in any machine learning workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fdbbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the dataset\n",
    "student_dropout = fetch_ucirepo(id=697)  # UCI ID for the dataset\n",
    "\n",
    "# Use the original DataFrame with the target column for PyCaret\n",
    "# If the target is not already a column, add it\n",
    "if hasattr(student_dropout.data, 'original'):\n",
    "    df = student_dropout.data.original.copy()\n",
    "else:\n",
    "    # fallback: merge features and targets\n",
    "    df = pd.concat([student_dropout.data.features, student_dropout.data.targets], axis=1)\n",
    "\n",
    "print('Data shape:', df.shape)\n",
    "print('First 5 rows:')\n",
    "display(df.head())\n",
    "\n",
    "# EDA (optional, for exploration only)\n",
    "print('\\nTarget value counts:')\n",
    "display(df['Target'].value_counts())\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(df.drop('Target', axis=1).corr(), cmap='coolwarm', annot=False)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f4599",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Data Preprocessing - Consistent Feature Names\n",
    "To avoid errors and ensure reproducibility, we standardize feature names and ensure all required features are present. This is critical for robust pipelines and model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_feature_names(df):\n",
    "    \"\"\"Ensure consistent feature naming across entire pipeline\"\"\"\n",
    "    df = df.copy()\n",
    "    # PyCaret's default name cleaning rules:\n",
    "    df.columns = [col.replace(' ', '_').replace('-', '_').replace('/', '_')\n",
    "                 .replace('(', '').replace(')', '').replace(\"'\", \"\").strip()\n",
    "                 for col in df.columns]\n",
    "    return df\n",
    "\n",
    "# Apply to original data\n",
    "df_clean = standardize_feature_names(df)\n",
    "\n",
    "# Verify all expected features are present\n",
    "required_features = [\n",
    "    'Marital_Status', 'Application_mode', 'Application_order', 'Course',\n",
    "    'Daytime_evening_attendance', 'Previous_qualification',\n",
    "    'Previous_qualification_grade', 'Nacionality',\n",
    "    'Mothers_qualification', 'Fathers_qualification',\n",
    "    'Mothers_occupation', 'Fathers_occupation', 'Admission_grade',\n",
    "    'Displaced', 'Educational_special_needs', 'Debtor',\n",
    "    'Tuition_fees_up_to_date', 'Gender', 'Scholarship_holder',\n",
    "    'Age_at_enrollment', 'International',\n",
    "    'Curricular_units_1st_sem_credited',\n",
    "    'Curricular_units_1st_sem_enrolled',\n",
    "    'Curricular_units_1st_sem_evaluations',\n",
    "    'Curricular_units_1st_sem_approved',\n",
    "    'Curricular_units_1st_sem_grade',\n",
    "    'Curricular_units_1st_sem_without_evaluations',\n",
    "    'Curricular_units_2nd_sem_credited',\n",
    "    'Curricular_units_2nd_sem_enrolled',\n",
    "    'Curricular_units_2nd_sem_evaluations',\n",
    "    'Curricular_units_2nd_sem_approved',\n",
    "    'Curricular_units_2nd_sem_grade',\n",
    "    'Curricular_units_2nd_sem_without_evaluations',\n",
    "    'Unemployment_rate', 'Inflation_rate', 'GDP'\n",
    "]\n",
    "\n",
    "# Ensure all features are present\n",
    "missing_features = [f for f in required_features if f not in df_clean.columns]\n",
    "if missing_features:\n",
    "    print(f\"Warning: {len(missing_features)} features missing. Adding with default values.\")\n",
    "    for f in missing_features:\n",
    "        df_clean[f] = 0  # Or appropriate default value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e909c65",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Model Development with Consistent Features\n",
    "We set up PyCaret with the cleaned data, enabling feature selection, multicollinearity removal, and class imbalance correction. This step prepares the data and environment for robust model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce4f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment with cleaned data\n",
    "exp = setup(\n",
    "    data=df_clean,\n",
    "    target='Target',\n",
    "    session_id=42,\n",
    "    fold_strategy='stratifiedkfold',\n",
    "    fold=5,\n",
    "    feature_selection=True,\n",
    "    remove_multicollinearity=True,\n",
    "    fix_imbalance=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train and finalize model\n",
    "best_model = compare_models(sort='F1')\n",
    "final_model = finalize_model(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282bbe60",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Consistent Evaluation and Visualization\n",
    "We evaluate the finalized model using confusion matrix, class report, and feature importance plots. This provides insight into model performance and areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3bfe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for consistent evaluation\n",
    "def safe_evaluation(model, data=None):\n",
    "    \"\"\"Handle feature name consistency during evaluation\"\"\"\n",
    "    if data is not None:\n",
    "        data = standardize_feature_names(data)\n",
    "        # Ensure all training features exist\n",
    "        for f in required_features:\n",
    "            if f not in data.columns and f != 'Target':\n",
    "                data[f] = 0\n",
    "    try:\n",
    "        # Plot confusion matrix\n",
    "        plot_model(model, plot='confusion_matrix', save=True)\n",
    "        # Plot class report\n",
    "        plot_model(model, plot='class_report', save=True)\n",
    "        # Plot feature importance\n",
    "        plot_model(model, plot='feature', save=True)\n",
    "        print(\"All evaluations completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation error: {str(e)}\")\n",
    "        # Fallback to SHAP feature importance\n",
    "        import shap\n",
    "        # Extract the underlying estimator from the pipeline\n",
    "        if hasattr(model, 'steps'):\n",
    "            estimator = model.steps[-1][1]\n",
    "        else:\n",
    "            estimator = model\n",
    "        explainer = shap.TreeExplainer(estimator)\n",
    "        shap_values = explainer.shap_values(df_clean.drop('Target', axis=1))\n",
    "        shap.summary_plot(shap_values, df_clean.drop('Target', axis=1), show=False)\n",
    "        plt.savefig('feature_importance_fallback.png')\n",
    "        plt.close()\n",
    "\n",
    "# Run evaluations\n",
    "safe_evaluation(final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc302b41",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## SHAP Analysis with Feature Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a7d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_shap_analysis(model, data):\n",
    "    \"\"\"Run SHAP analysis with feature name handling\"\"\"\n",
    "    data_clean = standardize_feature_names(data)\n",
    "    X = data_clean.drop('Target', axis=1)\n",
    "    # Ensure all features exist\n",
    "    for f in required_features:\n",
    "        if f not in X.columns and f != 'Target':\n",
    "            X[f] = 0\n",
    "    try:\n",
    "        import shap\n",
    "        # Extract the underlying estimator from the pipeline\n",
    "        if hasattr(model, 'steps'):\n",
    "            estimator = model.steps[-1][1]\n",
    "        else:\n",
    "            estimator = model\n",
    "        explainer = shap.TreeExplainer(estimator)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        if isinstance(shap_values, list):\n",
    "            for i, sv in enumerate(shap_values):\n",
    "                shap.summary_plot(sv, X, show=False)\n",
    "                plt.title(f'SHAP Summary - Class {i}')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'shap_class_{i}.png', dpi=300)\n",
    "                plt.close()\n",
    "        else:\n",
    "            shap.summary_plot(shap_values, X, show=False)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('shap_summary.png', dpi=300)\n",
    "            plt.close()\n",
    "        # Export values\n",
    "        if isinstance(shap_values, list):\n",
    "            for i, sv in enumerate(shap_values):\n",
    "                pd.DataFrame(sv, columns=X.columns).to_csv(f'shap_values_class_{i}.csv')\n",
    "        else:\n",
    "            pd.DataFrame(shap_values, columns=X.columns).to_csv('shap_values.csv')\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP failed: {str(e)}\")\n",
    "\n",
    "run_shap_analysis(final_model, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a945a854",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Model Tuning & Finalization\n",
    "We save the finalized model and required features for production use. This ensures reproducibility and enables future predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fed94f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Ensure model_outputs directory exists\n",
    "os.makedirs('model_outputs', exist_ok=True)\n",
    "\n",
    "# Save the final model in the proper directory\n",
    "save_model(final_model, 'model_outputs/final_student_dropout_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. When making predictions, ensure same preprocessing\n",
    "def predict_with_clean_features(model, data):\n",
    "    \"\"\"Wrapper function to ensure consistent feature names and all required features present\"\"\"\n",
    "    data_clean = standardize_feature_names(data)\n",
    "    for col in required_features:\n",
    "        if col not in data_clean.columns and col != 'Target':\n",
    "            data_clean[col] = 0  # Or appropriate default/imputation\n",
    "    # Reorder columns to match training\n",
    "    feature_cols = [col for col in required_features if col in data_clean.columns]\n",
    "    if 'Target' in data_clean.columns:\n",
    "        feature_cols.append('Target')\n",
    "    data_clean = data_clean[[col for col in data_clean.columns if col in feature_cols]]\n",
    "    return predict_model(model, data=data_clean)\n",
    "\n",
    "# 6. Usage example: predict on original data (or replace with new data)\n",
    "predictions = predict_with_clean_features(final_model, df)\n",
    "display(predictions[['Target', 'prediction_label', 'prediction_score']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afb83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "plot_model(final_model, plot='confusion_matrix')\n",
    "plot_model(final_model, plot='class_report')\n",
    "plot_model(final_model, plot='feature')\n",
    "\n",
    "# Save required features for production in the same directory\n",
    "with open('model_outputs/model_features.json', 'w') as f:\n",
    "    json.dump(required_features, f)\n",
    "\n",
    "# After model finalization, export feature importances\n",
    "try:\n",
    "    # Get the pipeline including preprocessing steps\n",
    "    pipeline = final_model\n",
    "    \n",
    "    # Get the preprocessed data\n",
    "    X_test = get_config('X_test')\n",
    "    y_test = get_config('y_test')\n",
    "    \n",
    "    # Calculate permutation importance on the pipeline (including preprocessing)\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    result = permutation_importance(\n",
    "        pipeline, \n",
    "        X_test, \n",
    "        y_test, \n",
    "        n_repeats=10, \n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "    \n",
    "    # Get the feature names after preprocessing\n",
    "    used_features = X_test.columns\n",
    "    \n",
    "    # Create and save importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': used_features,\n",
    "        'importance': result.importances_mean\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    importance_df.to_csv('model_outputs/feature_importance.csv', index=False)\n",
    "    print(\"Successfully exported permutation importance\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(\n",
    "        x='importance', \n",
    "        y='feature', \n",
    "        data=importance_df.head(20),\n",
    "        palette='viridis'\n",
    "    )\n",
    "    plt.title('Top 20 Most Important Features (Permutation Importance)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_outputs/feature_importance_plot.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to calculate feature importance: {str(e)}\")\n",
    "    # Fallback to simple feature importance if available\n",
    "    if hasattr(estimator, 'feature_importances_'):\n",
    "        try:\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': used_features[:len(estimator.feature_importances_)],\n",
    "                'importance': estimator.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            importance_df.to_csv('model_outputs/feature_importance_fallback.csv', index=False)\n",
    "            print(\"Exported simple feature importance as fallback\")\n",
    "        except:\n",
    "            print(\"Could not export any feature importance metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ebb4d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Prediction with Clean Features\n",
    "We define a function to preprocess new data and make predictions using the finalized model, ensuring consistency with the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. When making predictions, ensure same preprocessing\n",
    "def predict_with_clean_features(model, data):\n",
    "    \"\"\"Wrapper function to ensure consistent feature names and all required features present\"\"\"\n",
    "    data_clean = standardize_feature_names(data)\n",
    "    for col in required_features:\n",
    "        if col not in data_clean.columns and col != 'Target':\n",
    "            data_clean[col] = 0  # Or appropriate default/imputation\n",
    "    # Reorder columns to match training\n",
    "    feature_cols = [col for col in required_features if col in data_clean.columns]\n",
    "    if 'Target' in data_clean.columns:\n",
    "        feature_cols.append('Target')\n",
    "    data_clean = data_clean[[col for col in data_clean.columns if col in feature_cols]]\n",
    "    return predict_model(model, data=data_clean)\n",
    "\n",
    "# 6. Usage example: predict on original data (or replace with new data)\n",
    "predictions = predict_with_clean_features(final_model, df)\n",
    "display(predictions[['Target', 'prediction_label', 'prediction_score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c6c06",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Feature Importance Extraction\n",
    "We use permutation importance to robustly estimate feature importances, saving both a CSV and a plot. If permutation importance fails, we fall back to native feature importances if available. This step helps interpret the model and guide future feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f78a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After model finalization, export feature importances\n",
    "try:\n",
    "    # Get the pipeline including preprocessing steps\n",
    "    pipeline = final_model\n",
    "    \n",
    "    # Get the preprocessed data\n",
    "    X_test = get_config('X_test')\n",
    "    y_test = get_config('y_test')\n",
    "    \n",
    "    # Calculate permutation importance on the pipeline (including preprocessing)\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    result = permutation_importance(\n",
    "        pipeline, \n",
    "        X_test, \n",
    "        y_test, \n",
    "        n_repeats=10, \n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "    \n",
    "    # Get the feature names after preprocessing\n",
    "    used_features = X_test.columns\n",
    "    \n",
    "    # Create and save importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': used_features,\n",
    "        'importance': result.importances_mean\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    importance_df.to_csv('model_outputs/feature_importance.csv', index=False)\n",
    "    print(\"Successfully exported permutation importance\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(\n",
    "        x='importance', \n",
    "        y='feature', \n",
    "        data=importance_df.head(20),\n",
    "        palette='viridis'\n",
    "    )\n",
    "    plt.title('Top 20 Most Important Features (Permutation Importance)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_outputs/feature_importance_plot.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to calculate feature importance: {str(e)}\")\n",
    "    # Fallback to simple feature importance if available\n",
    "    if hasattr(estimator, 'feature_importances_'):\n",
    "        try:\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': used_features[:len(estimator.feature_importances_)],\n",
    "                'importance': estimator.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            importance_df.to_csv('model_outputs/feature_importance_fallback.csv', index=False)\n",
    "            print(\"Exported simple feature importance as fallback\")\n",
    "        except:\n",
    "            print(\"Could not export any feature importance metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886f82e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
